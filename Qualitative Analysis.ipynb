{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b56146dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import krippendorff\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624743fc",
   "metadata": {},
   "source": [
    "**Randomly sampling 30 examples for annotations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53443fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling completed and file saved as mdf_sample_30.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your dataset\n",
    "df_original = pd.read_csv('./extremism_data_final.csv')\n",
    "\n",
    "# Sampling according to labels\n",
    "class_extremist = df_original[df_original['Extremism_Label'] == 'EXTREMIST'].sample(n=15, random_state=42)\n",
    "class_non_extremist = df_original[df_original['Extremism_Label'] == 'NON_EXTREMIST'].sample(n=15, random_state=42)\n",
    "\n",
    "# Combine both samples\n",
    "sample_df = pd.concat([class_extremist , class_non_extremist])\n",
    "\n",
    "# Shuffle the final combined dataset \n",
    "sample_df = sample_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Add ID column\n",
    "sample_df.insert(0, 'ID', range(1, len(sample_df) + 1))\n",
    "\n",
    "# Save to CSV\n",
    "sample_df.to_csv('df_sample_30.csv', index=False)\n",
    "\n",
    "print(\"Sampling completed and file saved as mdf_sample_30.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee14104",
   "metadata": {},
   "source": [
    "**Inter-Rater Reliability(IRR) Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac9a869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "IRR analysis completed and results saved.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ===============================\n",
    "# 1. Load Annotated Dataset\n",
    "# ===============================\n",
    "\n",
    "df_annotated = pd.read_csv(\"mbd_annotation_sample_30.csv\")\n",
    "\n",
    "annotator_columns = [\n",
    "    \"Extremism_Label\",\n",
    "    \"Group_member1\",\n",
    "    \"Group_member2\",\n",
    "    \"Group_member3\",\n",
    "    \"Group_member4\",\n",
    "    \"Group_member5\"\n",
    "]\n",
    "\n",
    "# Remove rows with missing annotations\n",
    "df_annotated = df_annotated.dropna(subset=annotator_columns)\n",
    "\n",
    "# Converting labels to numeric as they are categorical\n",
    "for col in annotator_columns:\n",
    "    df_annotated[col] = df_annotated[col].astype(\"category\").cat.codes\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# 2. Percentage Agreement\n",
    "# ===============================\n",
    "\n",
    "def pairwise_agreement(df, col1, col2):\n",
    "    agreements = (df[col1] == df[col2]).sum()\n",
    "    total = len(df)\n",
    "    return round(agreements / total, 3)\n",
    "\n",
    "agreement_results = []\n",
    "\n",
    "for col1, col2 in combinations(annotator_columns, 2):\n",
    "    score = pairwise_agreement(df_annotated, col1, col2)\n",
    "    agreement_results.append({\n",
    "        \"Rater_1\": col1,\n",
    "        \"Rater_2\": col2,\n",
    "        \"Percent_Agreement\": score\n",
    "    })\n",
    "\n",
    "agreement_df = pd.DataFrame(agreement_results)\n",
    "\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# 3. Pairwise Krippendorff Alpha\n",
    "# ===============================\n",
    "\n",
    "def compute_pairwise_alpha(df, columns, level=\"nominal\"):\n",
    "    results = []\n",
    "    \n",
    "    for col1, col2 in combinations(columns, 2):\n",
    "        pair_data = [\n",
    "            df[col1].tolist(),\n",
    "            df[col2].tolist()\n",
    "        ]\n",
    "        \n",
    "        alpha = krippendorff.alpha(\n",
    "            reliability_data=pair_data,\n",
    "            level_of_measurement=level\n",
    "        )\n",
    "        \n",
    "        results.append({\n",
    "            \"Rater_1\": col1,\n",
    "            \"Rater_2\": col2,\n",
    "            \"Krippendorff_Alpha\": round(alpha, 3)\n",
    "        })\n",
    "        \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "alpha_df = compute_pairwise_alpha(df_annotated, annotator_columns, level=\"nominal\")\n",
    "\n",
    "# ===============================\n",
    "# 4. Save Results\n",
    "# ===============================\n",
    "\n",
    "agreement_df.to_csv(\"pairwise_percentage_agreement.csv\", index=False)\n",
    "alpha_df.to_csv(\"pairwise_krippendorff_alpha.csv\", index=False)\n",
    "\n",
    "print(\"\\nIRR analysis completed and results saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8eb5625",
   "metadata": {},
   "source": [
    "**Disagreement Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c3ad69fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Disagreement Analysis =====\n",
      "Total Examples: 30\n",
      "Disagreement Examples: 24\n",
      "Inter-Human Disagreement Rate: 0.8\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# Disagreement Analysis\n",
    "# ===============================\n",
    "\n",
    "human_annotators = [\n",
    "    \"Group_member1\",\n",
    "    \"Group_member2\",\n",
    "    \"Group_member3\",\n",
    "    \"Group_member4\",\n",
    "    \"Group_member5\"\n",
    "]\n",
    "\n",
    "# Count unique labels per row\n",
    "df_annotated[\"unique_labels\"] = df_annotated[human_annotators].nunique(axis=1)\n",
    "\n",
    "# Mark disagreements\n",
    "df_annotated[\"disagreement\"] = df_annotated[\"unique_labels\"] > 1\n",
    "\n",
    "# Extract disagreement examples\n",
    "disagreement_examples = df_annotated[df_annotated[\"disagreement\"] == True]\n",
    "\n",
    "# Calculate disagreement rate\n",
    "total_examples = len(df_annotated)\n",
    "num_disagreements = df_annotated[\"disagreement\"].sum()\n",
    "disagreement_rate = round(num_disagreements / total_examples, 3)\n",
    "\n",
    "print(\"\\n===== Disagreement Analysis =====\")\n",
    "print(f\"Total Examples: {total_examples}\")\n",
    "print(f\"Disagreement Examples: {num_disagreements}\")\n",
    "print(f\"Inter-Human Disagreement Rate: {disagreement_rate}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5236a2e3",
   "metadata": {},
   "source": [
    "**Examples that had the most inter-human disagreement for our group's sample set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f0acc446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples with highest disagreement: 10\n",
      "\n",
      "Examples with strongest disagreement (3 vs 2 splits):\n",
      "    ID                                   Original_Message  Extremism_Label  \\\n",
      "0    1  she isnt racist unlike your somalia bitch frie...                1   \n",
      "1    2  if i ever saw kendall jones in person i d kill...                1   \n",
      "3    4  i am in iraq as whatever speak your country is...                1   \n",
      "6    7  disabled people artfully the fucking scum of t...                1   \n",
      "7    8  if your fiance has a conversation with a bitch...                1   \n",
      "13  14  whatever do not blow up our wives and children...                0   \n",
      "14  15  never a doubt that our founders saw the writin...                0   \n",
      "20  21  in my opinion whatever should be afraid of the...                1   \n",
      "23  24  keyshia key whore is a bird gucci should have ...                1   \n",
      "25  26  ceasefire let s see how long those towel heads...                0   \n",
      "\n",
      "    Group_member1  Group_member2  Group_member3  Group_member4  Group_member5  \\\n",
      "0               0              1              1              1              0   \n",
      "1               0              0              1              1              0   \n",
      "3               1              0              0              1              0   \n",
      "6               0              1              1              0              0   \n",
      "7               0              0              0              1              1   \n",
      "13              0              1              1              1              0   \n",
      "14              1              1              0              1              0   \n",
      "20              0              0              1              1              1   \n",
      "23              0              0              0              1              1   \n",
      "25              1              1              0              0              0   \n",
      "\n",
      "    unique_labels  disagreement  disagreement_strength  \n",
      "0               2          True                    0.4  \n",
      "1               2          True                    0.4  \n",
      "3               2          True                    0.4  \n",
      "6               2          True                    0.4  \n",
      "7               2          True                    0.4  \n",
      "13              2          True                    0.4  \n",
      "14              2          True                    0.4  \n",
      "20              2          True                    0.4  \n",
      "23              2          True                    0.4  \n",
      "25              2          True                    0.4  \n"
     ]
    }
   ],
   "source": [
    "#Compute Disagreement Score\n",
    "\n",
    "def disagreement_strength(row):\n",
    "    counts = row[human_annotators].value_counts()\n",
    "    majority_count = counts.max()\n",
    "    total = len(human_annotators)\n",
    "    return 1 - (majority_count / total)\n",
    "\n",
    "df_annotated[\"disagreement_strength\"] = df_annotated.apply(disagreement_strength, axis=1)\n",
    "\n",
    "\n",
    "max_strength = df_annotated[\"disagreement_strength\"].max()\n",
    "\n",
    "strong_disagreement = df_annotated[df_annotated[\"disagreement_strength\"] == max_strength]\n",
    "print(\"Number of examples with highest disagreement:\", len(strong_disagreement))\n",
    "print(\"\\nExamples with strongest disagreement (3 vs 2 splits):\")\n",
    "print(strong_disagreement)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c9ae20",
   "metadata": {},
   "source": [
    "**How often did the majority of annotators agree with the original label**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4a148a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority agreement count: 16\n",
      "Total examples: 30\n",
      "Majority vs Original Agreement Rate: 0.533\n"
     ]
    }
   ],
   "source": [
    "#Compute Majority Vote\n",
    "df_annotated[\"majority_vote\"] = df_annotated[human_annotators].mode(axis=1)[0]\n",
    "\n",
    "#Compare Majority With Original Label\n",
    "df_annotated[\"majority_matches_original\"] = (\n",
    "    df_annotated[\"majority_vote\"] == df_annotated[\"Extremism_Label\"]\n",
    ")\n",
    "\n",
    "#Calculate Agreement Rate\n",
    "agreement_count = df_annotated[\"majority_matches_original\"].sum()\n",
    "\n",
    "total_examples = len(df_annotated)\n",
    "\n",
    "agreement_rate = round(agreement_count / total_examples, 3)\n",
    "\n",
    "print(\"Majority agreement count:\", agreement_count)\n",
    "print(\"Total examples:\", total_examples)\n",
    "print(\"Majority vs Original Agreement Rate:\", agreement_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1d74de",
   "metadata": {},
   "source": [
    "**Where did your group disagree with original labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d6a47eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Examples Where Group Disagreed With Original Label =====\n",
      "\n",
      "    ID  Extremism_Label  majority_vote\n",
      "1    2                1              0\n",
      "3    4                1              0\n",
      "6    7                1              0\n",
      "7    8                1              0\n",
      "9   10                0              1\n",
      "12  13                0              1\n",
      "13  14                0              1\n",
      "14  15                0              1\n",
      "16  17                0              1\n",
      "17  18                0              1\n",
      "21  22                1              0\n",
      "23  24                1              0\n",
      "24  25                1              0\n",
      "26  27                0              1\n"
     ]
    }
   ],
   "source": [
    "group_disagreements = df_annotated[df_annotated[\"majority_matches_original\"] == False]\n",
    "\n",
    "print(\"\\n===== Examples Where Group Disagreed With Original Label =====\\n\")\n",
    "print(group_disagreements[[\n",
    "    \"ID\", \n",
    "    \"Extremism_Label\", \n",
    "    \"majority_vote\"\n",
    "]])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PDA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
