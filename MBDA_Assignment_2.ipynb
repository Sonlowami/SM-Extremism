{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sonlowami/SM-Extremism/blob/main/MBDA_Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "pTQigtU9r7eK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kaggle Extremism Detection Challenge"
      ],
      "metadata": {
        "id": "m7NfPeTJrQzf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U krippendorff wordcloud"
      ],
      "metadata": {
        "id": "oLFkerCac0wv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FiCXVFeN8Iqc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import krippendorff\n",
        "from itertools import combinations\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tabulate import tabulate\n",
        "\n",
        "# Sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#A. Quantitative Analysis"
      ],
      "metadata": {
        "id": "7UhCLj028SpG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 1mzOPHeii_me8VuBNoo6BDVlxLNj9zXIM -O extremism_data_final.csv\n",
        "!gdown --id 1RETEMjmBma38HlRZuuB9NLjwvXJCRhu4 -O mbd_annotation_sample_30.csv"
      ],
      "metadata": {
        "id": "o8l-P84WXpUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the data\n",
        "df = pd.read_csv(\"./extremism_data_final.csv\")"
      ],
      "metadata": {
        "id": "1lRUCUo28bpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Statistics\n",
        "\n",
        "  - Assess dataset shape, size, class counts, and percentage distribution\n",
        "  - Check for duplicates and missing values (drop/impute with justification)\n",
        "  - Text characteristics: add text_length, word_count, avg_word_length columns\n",
        "  - Compare statistics for the new columns between classes\n",
        "  - Visualizations: Bar Chart (class distribution), Histograms (text characteristics)"
      ],
      "metadata": {
        "id": "kw7_ky0gFafo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Dataset Shape: {df.shape}\")\n",
        "print(f\"Number of rows: {df.shape[0]}\")\n",
        "print(f\"Number of columns: {df.shape[1]}\")\n",
        "print(f\"\\nColumn Names: {df.columns.tolist()}\")\n",
        "print(f\"\\nData Types:\\n{df.dtypes}\")\n",
        "print(f\"\\nFirst 5 rows:\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "Fcf3eFh8GCz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation:** The dataset contains **2,777 rows** and **2 columns** (`Original_Message` and `Extremism_Label`). Both columns are of string type, which is expected for a text classification task. The structure is straightforward: one feature column (the post text) and one target label column.\n"
      ],
      "metadata": {
        "id": "VrPojfviGPGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_counts = df['Extremism_Label'].value_counts()\n",
        "class_pct = df['Extremism_Label'].value_counts(normalize=True) * 100\n",
        "\n",
        "print(\"=== Class Distribution ===\")\n",
        "print(f\"\\nClass Counts:\\n{class_counts}\")\n",
        "print(f\"\\nPercentage Distribution:\\n{class_pct.round(2)}\")\n",
        "print(f\"\\nTotal samples: {len(df)}\")"
      ],
      "metadata": {
        "id": "-6Wksdp1Ggq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation:** The dataset is **fairly balanced**. NON_EXTREMIST posts account for **52.36%** (1,454 samples) while EXTREMIST posts account for **47.64%** (1,323 samples). The gap is only about 4.7%, which means we do not need to apply resampling techniques such as SMOTE or undersampling to address class imbalance. This balance is favorable for training an unbiased classifier.\n"
      ],
      "metadata": {
        "id": "e1q8-SH9GjQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "full_duplicates = df.duplicated().sum()\n",
        "text_duplicates = df.duplicated(subset=['Original_Message']).sum()\n",
        "\n",
        "print(\"=== Duplicate Analysis ===\")\n",
        "print(f\"Full row duplicates: {full_duplicates}\")\n",
        "print(f\"Text-only duplicates: {text_duplicates}\")\n",
        "\n",
        "if full_duplicates > 0 or text_duplicates > 0:\n",
        "    print(f\"\\nDropping {text_duplicates} text duplicates...\")\n",
        "    df = df.drop_duplicates(subset=['Original_Message'], keep='first')\n",
        "    print(f\"Dataset size after dropping duplicates: {df.shape[0]}\")\n",
        "    print(\"\\nJustification: Duplicates are dropped to prevent data leakage and bias.\")\n",
        "    print(\"Keeping duplicate texts would inflate the importance of repeated samples\")\n",
        "    print(\"and could lead to overfitting if the same text appears in train and test sets.\")\n",
        "else:\n",
        "    print(\"\\nNo duplicates found. The dataset is clean in this regard.\")"
      ],
      "metadata": {
        "id": "S-654Q1BGrIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation:** **No duplicates** were found, neither full-row duplicates nor text-only duplicates. Every post in the dataset is unique, so there is no risk of data leakage from duplicate texts appearing in both training and test splits.\n"
      ],
      "metadata": {
        "id": "XnwL73ZkGuEB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== Missing Values Analysis ===\")\n",
        "print(f\"\\nMissing values per column:\\n{df.isnull().sum()}\")\n",
        "print(f\"\\nTotal missing values: {df.isnull().sum().sum()}\")\n",
        "\n",
        "# Show the missing rows\n",
        "missing_text = df[df['Original_Message'].isnull()]\n",
        "if len(missing_text) > 0:\n",
        "    print(f\"\\nRows with missing text ({len(missing_text)} found):\")\n",
        "    print(missing_text)\n",
        "\n",
        "    # Drop rows with missing text\n",
        "    df = df.dropna(subset=['Original_Message'])\n",
        "    print(f\"\\nDataset size after handling missing values: {df.shape[0]}\")\n",
        "else:\n",
        "    print(\"\\nNo missing values found!\")\n",
        "\n",
        "# Confirm clean dataset\n",
        "print(f\"\\n=== Clean Dataset Summary ===\")\n",
        "print(f\"Final shape: {df.shape}\")\n",
        "print(f\"Final class distribution:\\n{df['Extremism_Label'].value_counts()}\")"
      ],
      "metadata": {
        "id": "pIg3XmrlG08W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation:** Only **1 missing value** was found at index 1228 (row 1230 in Excel), where the `Original_Message` is empty but labeled as NON_EXTREMIST. This row was **dropped** rather than imputed.\n",
        "\n",
        "**Justification for deletion:** The text column is our only input feature, so there is no meaningful way to impute or generate a social media post that does not exist. Filling it with an empty string or a placeholder like \"unknown\" would only introduce noise into the model. Since only 1 out of 2,777 rows is affected (**≈0.04%** of the data), the impact on dataset size is negligible. The cleaned dataset now has **2,776 rows**."
      ],
      "metadata": {
        "id": "qj648k6ZG5hQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['text_length'] = df['Original_Message'].str.len()\n",
        "df['word_count'] = df['Original_Message'].str.split().str.len()\n",
        "df['avg_word_length'] = df['text_length'] / df['word_count']\n",
        "\n",
        "print(\"=== New Columns Added ===\")\n",
        "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
        "print(f\"\\nSample of new columns:\")\n",
        "df[['Original_Message', 'text_length', 'word_count', 'avg_word_length']].head(10)"
      ],
      "metadata": {
        "id": "dsqjTdjdHClI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation:** Three new feature columns have been successfully added to the dataset:\n",
        "- `text_length`: Total number of characters in each post\n",
        "- `word_count`: Total number of words in each post\n",
        "- `avg_word_length`: Average number of characters per word (text_length / word_count)\n",
        "\n",
        "These columns allow us to quantitatively compare the structural properties of EXTREMIST vs NON_EXTREMIST posts."
      ],
      "metadata": {
        "id": "1eCyFUiWKsPC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== Text Characteristics: Overall Statistics ===\\n\")\n",
        "print(df[['text_length', 'word_count', 'avg_word_length']].describe().round(2))"
      ],
      "metadata": {
        "id": "u0hfNNn4J-dm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\\n=== Text Characteristics: By Class ===\\n\")\n",
        "class_stats = df.groupby('Extremism_Label')[['text_length', 'word_count', 'avg_word_length']].describe().round(2)\n",
        "class_stats"
      ],
      "metadata": {
        "id": "S2atIA9FKAQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Side-by-side mean comparison\n",
        "print(\"\\n\\n=== Mean Comparison ===\")\n",
        "mean_comparison = df.groupby('Extremism_Label')[['text_length', 'word_count', 'avg_word_length']].mean().round(2)\n",
        "print(mean_comparison)"
      ],
      "metadata": {
        "id": "H7zJfitbHMQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mean difference between classes\n",
        "extremist_means = df[df[\"Extremism_Label\"] == \"EXTREMIST\"][[\"text_length\", \"word_count\", \"avg_word_length\"]].mean()\n",
        "non_extremist_means = df[df[\"Extremism_Label\"] == \"NON_EXTREMIST\"][[\"text_length\", \"word_count\", \"avg_word_length\"]].mean()\n",
        "difference = extremist_means - non_extremist_means\n",
        "\n",
        "comparison = pd.DataFrame({\n",
        "    \"EXTREMIST\": extremist_means.round(2),\n",
        "    \"NON_EXTREMIST\": non_extremist_means.round(2),\n",
        "    \"Difference\": difference.round(2)\n",
        "})\n",
        "print(\"=== Class Comparison: Mean Differences ===\")\n",
        "print()"
      ],
      "metadata": {
        "id": "yvwTzUJKHRtu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation:** Comparing the mean text characteristics between classes:\n",
        "\n",
        "| Metric | EXTREMIST | NON_EXTREMIST | Difference |\n",
        "|--------|-----------|---------------|------------|\n",
        "| Text Length | 134.56 chars | 119.39 chars | **+15.17** |\n",
        "| Word Count | 24.50 words | 22.19 words | **+2.31** |\n",
        "| Avg Word Length | 5.47 chars | 5.41 chars | **+0.06** |\n",
        "\n",
        "**Key Insights:**\n",
        "- EXTREMIST posts are on average **15 characters longer** and use about **2 more words** than NON_EXTREMIST posts.\n",
        "- The average word length difference is negligible (**+0.06 chars**), meaning vocabulary complexity is nearly identical across both classes.\n",
        "- This tells us that **post length alone is not a reliable feature** for classification. The model will need to learn actual linguistic patterns and word-level content to distinguish between the two classes."
      ],
      "metadata": {
        "id": "nxoVZJaUHX-i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(8, 5))\n",
        "\n",
        "colors = ['#e74c3c', '#2ecc71']  # red for extremist, green for non-extremist\n",
        "class_counts = df['Extremism_Label'].value_counts()\n",
        "\n",
        "bars = ax.bar(class_counts.index, class_counts.values, color=colors, edgecolor='black', width=0.5)\n",
        "\n",
        "# Add count labels on bars\n",
        "for bar, count, pct in zip(bars, class_counts.values, (class_counts / len(df) * 100)):\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 20,\n",
        "            f'{count}\\n({pct:.1f}%)', ha='center', va='bottom', fontweight='bold', fontsize=12)\n",
        "\n",
        "ax.set_title('Class Distribution: EXTREMIST vs NON_EXTREMIST', fontsize=14, fontweight='bold')\n",
        "ax.set_xlabel('Class Label', fontsize=12)\n",
        "ax.set_ylabel('Number of Posts', fontsize=12)\n",
        "ax.set_ylim(0, max(class_counts.values) * 1.2)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fUDwFW0rHgTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation:** The bar chart confirms a **near-balanced distribution** between the two classes. NON_EXTREMIST (52.3%) holds a slight majority over EXTREMIST (47.7%), but the difference is minimal. This visual representation supports our earlier statistical finding that no class rebalancing is required.\n"
      ],
      "metadata": {
        "id": "EHGoIo35HoAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "features = ['text_length', 'word_count', 'avg_word_length']\n",
        "titles = ['Text Length (Character Count)', 'Word Count', 'Average Word Length']\n",
        "colors_map = {'EXTREMIST': '#e74c3c', 'NON_EXTREMIST': '#2ecc71'}\n",
        "\n",
        "for idx, (feature, title) in enumerate(zip(features, titles)):\n",
        "    ax = axes[idx]\n",
        "    for label, color in colors_map.items():\n",
        "        subset = df[df['Extremism_Label'] == label][feature]\n",
        "        ax.hist(subset, bins=30, alpha=0.6, label=label, color=color, edgecolor='black')\n",
        "\n",
        "    ax.set_title(title, fontsize=13, fontweight='bold')\n",
        "    ax.set_xlabel(feature, fontsize=11)\n",
        "    ax.set_ylabel('Frequency', fontsize=11)\n",
        "    ax.legend()\n",
        "\n",
        "plt.suptitle('Text Characteristics Distribution by Class', fontsize=15, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IuiYFdCvHtWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation:** The histograms reveal the following patterns:\n",
        "\n",
        "1. **Text Length and Word Count:** Both classes show **right-skewed distributions**. The majority of posts are short (under 200 characters / 30 words), with a long tail of outliers extending to around 1,400 characters / 280 words. The distributions overlap heavily, confirming that post length is not a strong differentiator between classes.\n",
        "\n",
        "2. **Average Word Length:** Both classes follow a roughly **normal distribution** centered around 5 to 6 characters per word, with nearly identical shapes. This confirms that vocabulary complexity is comparable across both classes.\n",
        "\n",
        "**Conclusion:** The structural characteristics (length, word count, word complexity) of EXTREMIST and NON_EXTREMIST posts are very similar. Effective classification will therefore depend on **content-level features**, specifically the actual words and phrases used, rather than surface-level text statistics. This motivates the use of TF-IDF vectorization in the ML Baseline Model (Section C)."
      ],
      "metadata": {
        "id": "rmOyGjdmHxgx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linguistic Analysis\n",
        "\n",
        "      - Find the top 20 most occurying words\n",
        "      - Find the top 20 most occurying words in the extremist posts\n",
        "      - Find the top 20 most occurying words in the non-extremist posts\n",
        "      - Plot the word clouds for the extreme and non-extremist posts"
      ],
      "metadata": {
        "id": "CgeDnUUSfsMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = df['Original_Message'].str.split(expand=True).stack()\n",
        "top20 = words.value_counts().head(20)\n",
        "xtreme = df[df['Extremism_Label']=='EXTREMIST']['Original_Message'].str.split(expand=True).stack()\n",
        "unxtreme = df[df['Extremism_Label']=='NON_EXTREMIST']['Original_Message'].str.split(expand=True).stack()"
      ],
      "metadata": {
        "id": "p8ghu5NDX5-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_counts(counts, title):\n",
        "  print(title)\n",
        "  print(\"_\" * 40)\n",
        "\n",
        "  # Convert the Series to a list of lists for tabulate\n",
        "  table_data = [[word, count] for word, count in counts.items()]\n",
        "\n",
        "  print(tabulate(table_data, headers=[\"Word\", \"Frequency\"], tablefmt=\"fancy\"))"
      ],
      "metadata": {
        "id": "dAde7l2OY7HD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_counts(top20, \"Top 20 Frequent Words\")"
      ],
      "metadata": {
        "id": "K4dbF8J3YGmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_counts(xtreme.value_counts().head(20), \"Top 20 Frequent Words in Extremist Posts\")"
      ],
      "metadata": {
        "id": "Zd5PnAucYKaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_counts(unxtreme.value_counts().head(20), \"Top 20 Frequent Words in non Extremist Posts\")"
      ],
      "metadata": {
        "id": "f78YMvkfYMfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In both cases, there is a low-value english words like 'is', 'the', 'in', 'are', and others. To get a better feel for words that are relevant in extremist and non-extremist context, we can use a word cloud for each of the classes.\n",
        "\n",
        "As seen in the word cloud below, words like **kill** and **let** are associated with extremism while words like **bitch** and **fucking** are associated with non-extremism. This is surprising because the words highly associated with non-extremism are themselves vulgar. Moreover, if a user uses one of these words in an hate post, the weight given to them might sway classification models into misclassifying the message."
      ],
      "metadata": {
        "id": "tYPdXvkMtwvu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# word clouds for the extremist posts\n",
        "text = \" \".join(xtreme.values)\n",
        "xtreme_cloud = WordCloud(width=1000, height=800, background_color='white', collocations=False).generate(text)\n",
        "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(14, 8))\n",
        "ax[0].imshow(xtreme_cloud, interpolation='bilinear')\n",
        "ax[0].set_title(\"Common Words in the Extreme Statements\")\n",
        "ax[0].axis('off')\n",
        "\n",
        "# word clouds for the non-extremist posts\n",
        "text = \" \".join(unxtreme.values)\n",
        "unxtreme_cloud = WordCloud(width=1000, height=800, background_color='white', collocations=False).generate(text)\n",
        "ax[1].imshow(unxtreme_cloud, interpolation='bilinear')\n",
        "ax[1].set_title(\"Common Words in the Non-Extreme Statements\")\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "VT4jFn2qUAwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# B. Qualitative Analysis"
      ],
      "metadata": {
        "id": "oGPFfOP98dnS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Randomly sampling 30 examples for annotations"
      ],
      "metadata": {
        "id": "Jt_m6x2rdrbu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your dataset\n",
        "df_original = pd.read_csv('./extremism_data_final.csv')\n",
        "\n",
        "# Sampling according to labels\n",
        "class_extremist = df_original[df_original['Extremism_Label'] == 'EXTREMIST'].sample(n=15, random_state=42)\n",
        "class_non_extremist = df_original[df_original['Extremism_Label'] == 'NON_EXTREMIST'].sample(n=15, random_state=42)\n",
        "\n",
        "# Combine both samples\n",
        "sample_df = pd.concat([class_extremist , class_non_extremist])\n",
        "\n",
        "# Shuffle the final combined dataset\n",
        "sample_df = sample_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Add ID column\n",
        "sample_df.insert(0, 'ID', range(1, len(sample_df) + 1))\n",
        "\n",
        "# Save to CSV\n",
        "sample_df.to_csv('df_sample_30.csv', index=False)\n",
        "\n",
        "print(\"Sampling completed and file saved as df_sample_30.csv.csv\")"
      ],
      "metadata": {
        "id": "AJ-9PE_h8kMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inter-Rater Reliability(IRR) Analysis"
      ],
      "metadata": {
        "id": "Rahwz2hIgKqk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# 1. Load Annotated Dataset\n",
        "# ===============================\n",
        "\n",
        "df_annotated = pd.read_csv(\"mbd_annotation_sample_30.csv\")\n",
        "\n",
        "annotator_columns = [\n",
        "    \"Extremism_Label\",\n",
        "    \"Group_member1\",\n",
        "    \"Group_member2\",\n",
        "    \"Group_member3\",\n",
        "    \"Group_member4\",\n",
        "    \"Group_member5\"\n",
        "]\n",
        "\n",
        "# Remove rows with missing annotations\n",
        "df_annotated = df_annotated.dropna(subset=annotator_columns)\n",
        "\n",
        "# Converting labels to numeric as they are categorical\n",
        "for col in annotator_columns:\n",
        "    df_annotated[col] = df_annotated[col].astype(\"category\").cat.codes\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# 2. Percentage Agreement\n",
        "# ===============================\n",
        "\n",
        "def pairwise_agreement(df, col1, col2):\n",
        "    agreements = (df[col1] == df[col2]).sum()\n",
        "    total = len(df)\n",
        "    return round(agreements / total, 3)\n",
        "\n",
        "agreement_results = []\n",
        "\n",
        "for col1, col2 in combinations(annotator_columns, 2):\n",
        "    score = pairwise_agreement(df_annotated, col1, col2)\n",
        "    agreement_results.append({\n",
        "        \"Rater_1\": col1,\n",
        "        \"Rater_2\": col2,\n",
        "        \"Percent_Agreement\": score * 100\n",
        "    })\n",
        "\n",
        "agreement_df = pd.DataFrame(agreement_results)\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# 3. Pairwise Krippendorff Alpha\n",
        "# ===============================\n",
        "\n",
        "def compute_pairwise_alpha(df, columns, level=\"nominal\"):\n",
        "    results = []\n",
        "\n",
        "    for col1, col2 in combinations(columns, 2):\n",
        "        pair_data = [\n",
        "            df[col1].tolist(),\n",
        "            df[col2].tolist()\n",
        "        ]\n",
        "\n",
        "        alpha = krippendorff.alpha(\n",
        "            reliability_data=pair_data,\n",
        "            level_of_measurement=level\n",
        "        )\n",
        "\n",
        "        results.append({\n",
        "            \"Rater_1\": col1,\n",
        "            \"Rater_2\": col2,\n",
        "            \"Krippendorff_Alpha\": round(alpha, 3)\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "alpha_df = compute_pairwise_alpha(df_annotated, annotator_columns, level=\"nominal\")\n",
        "\n",
        "# ===============================\n",
        "# 4. Save Results\n",
        "# ===============================\n",
        "\n",
        "agreement_df.to_csv(\"pairwise_percentage_agreement.csv\", index=False)\n",
        "alpha_df.to_csv(\"pairwise_krippendorff_alpha.csv\", index=False)\n",
        "\n",
        "print(\"\\nIRR analysis completed and results saved.\")\n"
      ],
      "metadata": {
        "id": "cj7goP0DZn8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Disagreement Analysis"
      ],
      "metadata": {
        "id": "62IAKyDClGLF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# Disagreement Analysis\n",
        "# ===============================\n",
        "\n",
        "human_annotators = [\n",
        "    \"Group_member1\",\n",
        "    \"Group_member2\",\n",
        "    \"Group_member3\",\n",
        "    \"Group_member4\",\n",
        "    \"Group_member5\"\n",
        "]\n",
        "\n",
        "# Count unique labels per row\n",
        "df_annotated[\"unique_labels\"] = df_annotated[human_annotators].nunique(axis=1)\n",
        "\n",
        "# Mark disagreements\n",
        "df_annotated[\"disagreement\"] = df_annotated[\"unique_labels\"] > 1\n",
        "\n",
        "# Extract disagreement examples\n",
        "disagreement_examples = df_annotated[df_annotated[\"disagreement\"] == True]\n",
        "\n",
        "# Calculate disagreement rate\n",
        "total_examples = len(df_annotated)\n",
        "num_disagreements = df_annotated[\"disagreement\"].sum()\n",
        "disagreement_rate = round(num_disagreements / total_examples, 3)\n",
        "\n",
        "print(\"\\n===== Disagreement Analysis =====\")\n",
        "print(f\"Total Examples: {total_examples}\")\n",
        "print(f\"Disagreement Examples: {num_disagreements}\")\n",
        "print(f\"Inter-Human Disagreement Rate: {disagreement_rate}\")"
      ],
      "metadata": {
        "id": "2R2Zr-5jaFs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Examples that had the most inter-human disagreement for our group's sample set"
      ],
      "metadata": {
        "id": "Jhxef6_hlNKJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Compute Disagreement Score\n",
        "\n",
        "def disagreement_strength(row):\n",
        "    counts = row[human_annotators].value_counts()\n",
        "    majority_count = counts.max()\n",
        "    total = len(human_annotators)\n",
        "    return 1 - (majority_count / total)\n",
        "\n",
        "df_annotated[\"disagreement_strength\"] = df_annotated.apply(disagreement_strength, axis=1)\n",
        "\n",
        "\n",
        "max_strength = df_annotated[\"disagreement_strength\"].max()\n",
        "\n",
        "strong_disagreement = df_annotated[df_annotated[\"disagreement_strength\"] == max_strength]\n",
        "print(\"Number of examples with highest disagreement:\", len(strong_disagreement))\n",
        "print(\"\\nExamples with strongest disagreement (3 vs 2 splits):\")\n",
        "strong_disagreement\n"
      ],
      "metadata": {
        "id": "c4JVbZwLaU3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How often did the majority of annotators agree with the original label"
      ],
      "metadata": {
        "id": "4xPZr_mflTWl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Compute Majority Vote\n",
        "df_annotated[\"majority_vote\"] = df_annotated[human_annotators].mode(axis=1)[0]\n",
        "\n",
        "#Compare Majority With Original Label\n",
        "df_annotated[\"majority_matches_original\"] = (\n",
        "    df_annotated[\"majority_vote\"] == df_annotated[\"Extremism_Label\"]\n",
        ")\n",
        "\n",
        "#Calculate Agreement Rate\n",
        "agreement_count = df_annotated[\"majority_matches_original\"].sum()\n",
        "\n",
        "total_examples = len(df_annotated)\n",
        "\n",
        "agreement_rate = round(agreement_count / total_examples, 3)\n",
        "\n",
        "print(\"Majority agreement count:\", agreement_count)\n",
        "print(\"Total examples:\", total_examples)\n",
        "print(\"Majority vs Original Agreement Rate:\", agreement_rate)\n"
      ],
      "metadata": {
        "id": "Eu-gx9cWc4xx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Where our group disagreed with original labels"
      ],
      "metadata": {
        "id": "XhZMzvFQlYbI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "group_disagreements = df_annotated[df_annotated[\"majority_matches_original\"] == False]\n",
        "\n",
        "print(\"\\n===== Examples Where Group Disagreed With Original Label =====\\n\")\n",
        "group_disagreements[[\n",
        "    \"ID\",\n",
        "    \"Original_Message\",\n",
        "    \"Extremism_Label\",\n",
        "    \"majority_vote\"\n",
        "]]\n"
      ],
      "metadata": {
        "id": "6kHbczoKdCa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "group_disagreements[[\n",
        "    \"ID\",\n",
        "    \"Original_Message\",\n",
        "    \"Extremism_Label\",\n",
        "    \"majority_vote\"\n",
        "]].to_csv(\"group_disagreements.csv\", index=False)"
      ],
      "metadata": {
        "id": "gnRFLflkY8gv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# C. ML Baseline Model"
      ],
      "metadata": {
        "id": "xSF0Oi5z835E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Features and Labels"
      ],
      "metadata": {
        "id": "Sm7qXNzEaoNi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df['Original_Message'] # Independent variable\n",
        "y = df['Extremism_Label'] # Dependent variable"
      ],
      "metadata": {
        "id": "_cc4durfaqev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stratified Train / Validation / Test Split (70/15/15)"
      ],
      "metadata": {
        "id": "ATjzed-5as4u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into training (70%), 30% sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.30,\n",
        "    stratify=y,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Further split the 30% set into validation (15%) and test (15%) sets\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp,\n",
        "    test_size=0.50,\n",
        "    stratify=y_temp,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"Training set size:\", len(X_train))\n",
        "print(\"Validation set size:\", len(X_val))\n",
        "print(\"Test set size:\", len(X_test))"
      ],
      "metadata": {
        "id": "-piFEg2xavte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check class balance\n",
        "print(\"\\nTrain class distribution:\\n\", y_train.value_counts(normalize=True))\n",
        "print(\"\\nValidation class distribution:\\n\", y_val.value_counts(normalize=True))\n",
        "print(\"\\nTest class distribution:\\n\", y_test.value_counts(normalize=True))"
      ],
      "metadata": {
        "id": "8m6VHEYzapew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observation: The dataset is nearly balanced, across training, validation, and test sets. This means class imbalance is minimal, metrics like accuracy and F1 are reliable, and stratified splits have preserved class proportions. Using `class_weight=\"balanced\"` is a safe precaution, but the model can already learn both classes effectively.\n"
      ],
      "metadata": {
        "id": "FDeNOYipSK2G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TF-IDF Vectorization (Default Parameters Only)"
      ],
      "metadata": {
        "id": "BKOsD4DOa090"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize TF-IDF Vectorizer (default parameters)\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit on training data\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "\n",
        "# Transform validation and test\n",
        "X_val_tfidf = vectorizer.transform(X_val)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Check feature size\n",
        "print(\"Number of features:\", X_train_tfidf.shape[1])"
      ],
      "metadata": {
        "id": "soYapEX_9BrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Logistic Regression (Default Parameters)"
      ],
      "metadata": {
        "id": "iLohypQna5Hc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Logistic Regression model with default parameters\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Fit the model on the training data\n",
        "model.fit(X_train_tfidf, y_train)"
      ],
      "metadata": {
        "id": "0iXXfV5ea7AF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate on Validation Set"
      ],
      "metadata": {
        "id": "OyVE7NeZa9UX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on validation set\n",
        "y_val_pred = model.predict(X_val_tfidf)\n",
        "\n",
        "# Classification Report\n",
        "print(\"Classification Report (Validation Set):\\n\")\n",
        "print(classification_report(y_val, y_val_pred))"
      ],
      "metadata": {
        "id": "osxsjZx4a_iL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observation:\n",
        "\n",
        "* **EXTREMIST:** Precision 0.83, recall 0.79, F1-score 0.81. The model predicts EXTREMIST correctly most of the time but misses about 21% of extremist samples.\n",
        "* **NON_EXTREMIST:** Precision 0.81, recall 0.85, F1-score 0.83. The model identifies non-extremist samples slightly better than extremist ones.\n",
        "* **Overall accuracy:** 0.82, with macro and weighted F1 both at 0.82, indicating balanced performance across classes.\n",
        "\n",
        "**Interpretation:**\n",
        "The model is performing reasonably well, but EXTREMIST recall is slightly lower than NON_EXTREMIST, meaning some extremist content is being missed.\n"
      ],
      "metadata": {
        "id": "yxRfqNwJelkX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Confusion Matrix"
      ],
      "metadata": {
        "id": "p0x9LvbJbD6R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(y_val, y_val_pred)\n",
        "\n",
        "plt.figure()\n",
        "sns.heatmap(cm, annot=True, fmt='d',\n",
        "            xticklabels=model.classes_,\n",
        "            yticklabels=model.classes_)\n",
        "\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix - Validation Set\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PJCLlhXxbFn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observation:\n",
        "* **EXTREMIST class:** 156 correctly predicted, 42 misclassified as NON_EXTREMIST.\n",
        "* **NON_EXTREMIST class:** 185 correctly predicted, 33 misclassified as EXTREMIST."
      ],
      "metadata": {
        "id": "jvsV-5ffewD_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save Validation Errors (Needed for Part D)"
      ],
      "metadata": {
        "id": "XliVEIPXbIuq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame to analyze misclassifications\n",
        "val_results = pd.DataFrame({\n",
        "    \"text\": X_val,\n",
        "    \"true_label\": y_val,\n",
        "    \"predicted_label\": y_val_pred\n",
        "})\n",
        "\n",
        "# All misclassifications\n",
        "errors = val_results[val_results[\"true_label\"] != val_results[\"predicted_label\"]]\n",
        "\n",
        "# False Positives\n",
        "false_positives = val_results[\n",
        "    (val_results[\"true_label\"] == \"NON_EXTREMIST\") & (val_results[\"predicted_label\"] == \"EXTREMIST\")\n",
        "]\n",
        "\n",
        "# False Negatives\n",
        "false_negatives = val_results[\n",
        "    (val_results[\"true_label\"] == \"EXTREMIST\") & (val_results[\"predicted_label\"] == \"NON_EXTREMIST\")\n",
        "]\n",
        "\n",
        "print(\"Total misclassifications:\", len(errors))\n",
        "print(\"False Positives:\", len(false_positives))\n",
        "print(\"False Negatives:\", len(false_negatives))\n",
        "\n",
        "# Add error type column for manual review\n",
        "errors = errors.copy()\n",
        "errors[\"error_type\"] = errors.apply(\n",
        "    lambda r: \"FP\" if (r.true_label == \"NON_EXTREMIST\" and r.predicted_label == \"EXTREMIST\") else \"FN\",\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# Save to CSV\n",
        "errors.to_csv(\"errors_validation_10.csv\", index=False)"
      ],
      "metadata": {
        "id": "l7X84xT0bKQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# D. Required Error Analysis"
      ],
      "metadata": {
        "id": "NAWUja329CiH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Suggested Improvements"
      ],
      "metadata": {
        "id": "ZbNcnP5YPCvq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. TF-IDF + LinearSVC with Raw Term Frequency and Bigram Features\n",
        "\n",
        "\n",
        "**Description:**\n",
        "This approach combines **TF-IDF vectorization** using **unigrams and bigrams** (`ngram_range=(1,2)`) with **raw term frequencies** (`sublinear_tf=False`) and feeds the resulting high-dimensional sparse features into a **Linear Support Vector Classifier** (`LinearSVC`). The `class_weight=\"balanced\"` ensures that minority classes, like EXTREMIST, are given proportionally more importance. This setup captures both the presence and intensity of words and short phrases, enabling better separation of classes in text classification tasks, particularly when repeated or structured expressions carry meaningful signals."
      ],
      "metadata": {
        "id": "2wV3h-XwPIAb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "vectorizer = TfidfVectorizer(\n",
        "    ngram_range=(1,2),       # Include unigrams and bigrams\n",
        "    min_df=2,                # Only include terms that appear in at least 2 documents\n",
        "    max_df=0.95,             # Ignore terms that appear in more than 95% of documents\n",
        "    sublinear_tf=False       # Use raw term frequencies instead of sublinear scaling\n",
        ")\n",
        "\n",
        "# Fit on training data\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "\n",
        "# Transform validation and test\n",
        "X_val_tfidf = vectorizer.transform(X_val)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Check feature size\n",
        "print(\"Number of features:\", X_train_tfidf.shape[1])\n",
        "\n",
        "model = LinearSVC(class_weight=\"balanced\")\n",
        "model.fit(X_train_tfidf, y_train)"
      ],
      "metadata": {
        "id": "yJvuFlDJ9Hiu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions\n",
        "y_val_pred = model.predict(X_val_tfidf)\n",
        "\n",
        "# Classification Report\n",
        "print(\"Classification Report (Validation Set):\\n\")\n",
        "print(classification_report(y_val, y_val_pred))"
      ],
      "metadata": {
        "id": "kCvadVt1ABR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Overall Accuracy:** This improved from 82% to 84%, a modest but meaningful 2-point gain on the same validation set.\n",
        "\n",
        "**EXTREMIST class:** This is where the most interesting shift happened. Before, precision was slightly higher (0.83 vs 0.82 after), but recall was to 0.79. This means the pre-adjustment model was *missing* more actual extremist content, catching only 79% of true positives versus 85% now. For a content safety use case, this is the most critical improvement. The F1 reflects this: 0.81 → 0.84.\n",
        "\n",
        "**NON_EXTREMIST class:** The direction flips here. Before adjustments, recall was higher (0.85 vs 0.83 after), meaning the old model was more conservative, more inclined to label things as non-extremist. After adjustments, precision improved (0.81 → 0.86), so when the model clears something as non-extremist, it's more confident in that call.\n",
        "\n",
        "**The core trade-off:** The adjustments essentially shifted the model's decision boundary — it became more aggressive at catching extremist content (higher extremist recall) at the cost of slightly more false positives on that class (lower extremist precision) and slightly fewer non-extremist recalls."
      ],
      "metadata": {
        "id": "YiOmbLZVhF86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(y_val, y_val_pred)\n",
        "\n",
        "plt.figure()\n",
        "sns.heatmap(cm, annot=True, fmt='d',\n",
        "            xticklabels=model.classes_,\n",
        "            yticklabels=model.classes_)\n",
        "\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix - Validation Set\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "A_RSkNgDADQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observation:\n",
        "* **EXTREMIST class:** 168 correctly predicted, 30 misclassified as NON_EXTREMIST.\n",
        "* **NON_EXTREMIST class:** 182 correctly predicted, 36 misclassified as EXTREMIST."
      ],
      "metadata": {
        "id": "XquSt68ff5Og"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_results = pd.DataFrame({\n",
        "    \"text\": X_val,\n",
        "    \"true_label\": y_val,\n",
        "    \"predicted_label\": y_val_pred\n",
        "})\n",
        "\n",
        "# All misclassifications\n",
        "errors = val_results[val_results[\"true_label\"] != val_results[\"predicted_label\"]]\n",
        "\n",
        "# False Positives\n",
        "false_positives = val_results[\n",
        "    (val_results[\"true_label\"] == \"NON_EXTREMIST\") & (val_results[\"predicted_label\"] == \"EXTREMIST\")\n",
        "]\n",
        "\n",
        "# False Negatives\n",
        "false_negatives = val_results[\n",
        "    (val_results[\"true_label\"] == \"EXTREMIST\") & (val_results[\"predicted_label\"] == \"NON_EXTREMIST\")\n",
        "]\n",
        "\n",
        "print(\"Total misclassifications:\", len(errors))\n",
        "print(\"False Positives:\", len(false_positives))\n",
        "print(\"False Negatives:\", len(false_negatives))"
      ],
      "metadata": {
        "id": "_WOS5nY5NnG0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}